# Ex.No: 2 Evaluation of 2024 Prompting Tools Across Diverse AI Platforms: ChatGPT, Claude, Bard, Cohere Command, and Meta

**Aim:**
To systematically compare the capabilities of five leading generative AI models—ChatGPT, Claude, Bard, Cohere, and Meta AI—by analyzing their performance in varied prompt-based tasks across academic, technical, and creative domains.

```
Name: Sivaramakrishnan B
Reg.No: 212222110044

```

**Table of Contents:**

1. Introduction
2. Overview of AI Platforms
3. Research Objective
4. Tools and Technologies Used
5. Task Selection
6. Prompt Engineering Strategy
7. Experimental Design
8. Evaluation Metrics
9. Performance Matrix (Table)
10. Output Sample Snapshots
11. Comparative Graphs
12. Observations and Insights
13. Challenges Faced
14. Ethical Considerations
15. Summary of Findings
16. Conclusion
17. Future Scope

---

### 1. Introduction:
The rise of generative AI has led to the proliferation of models capable of generating human-like text. This report presents a structured study comparing five prominent tools through task-based evaluation.

### 2. Overview of AI Platforms:

* **ChatGPT (GPT-4)** – Known for fluency and contextual awareness.
* **Claude (Anthropic)** – Noted for its alignment and safety-focused responses.
* **Bard (Google PaLM 2)** – Incorporates live web data and versatile knowledge.
* **Cohere Command R+** – Specializes in summarization and structured output.
* **Meta AI (LLaMA 2)** – Emphasizes open-source innovation and creativity.

### 3. Research Objective:
To assess model performance in terms of accuracy, coherence, creativity, contextual awareness, and prompt relevance.

### 4. Tools and Technologies Used:

| Model   | Developer | Year | Interface Used   |
| ------- | --------- | ---- | ---------------- |
| ChatGPT | OpenAI    | 2023 | ChatGPT UI       |
| Claude  | Anthropic | 2024 | Claude Web App   |
| Bard    | Google    | 2024 | Bard Workspace   |
| Cohere  | Cohere AI | 2024 | Cohere Console   |
| Meta AI | Meta      | 2024 | HuggingFace Demo |

### 5. Task Selection:
Tasks included text summarization, logic puzzles, creative writing, code generation, and factual Q\&A, chosen to reveal model behavior across varied domains.

### 6. Prompt Engineering Strategy:
Each model was given the same prompt under identical conditions. Prompts were designed to be neutral and assess multi-domain performance.

---
<img src="https://github.com/user-attachments/assets/73bb2d8e-9072-4af1-97d5-e0e8156d2ba2" width="500" />



### 7. Experimental Design:
Sessions were conducted with consistent internet settings. Each session's response was logged with timestamps and screenshots.

### 8. Evaluation Metrics:
Models were scored (0–5 scale) on:

* Factual Accuracy
* Coherence
* Contextual Understanding
* Creativity
* Relevance to Prompt

### 9. Performance Matrix:


| Feature                  | ChatGPT | Claude | Bard | Cohere Command | Meta (LLaMA) |
| ------------------------ | ------- | ------ | ---- | -------------- | ------------ |
| Supports CoT             | ✅      | ✅     | ✅   | ❌             | ✅           |
| Few-shot Quality         | High    | Medium | High | Medium         | Low          |
| Output Clarity           | Very High | High  | Medium | Medium       | Medium       |
| Creative Reasoning       | ✅      | ✅     | ✅   | ❌             | ✅           |
| Domain Adaptability      | ✅      | ✅     | ✅   | ✅             | ❌           |


### 10. Output Sample Snapshots:
Screenshots and textual samples from each model’s output were collected for qualitative analysis.

### 11. Comparative Graphs:
Bar and radar charts were used to visualize the overall performance of each model across all criteria.

### 12. Observations and Insights:

* ChatGPT displayed the best overall performance.
* Claude offered logical depth and cautious tone.
* Bard excelled in current data-based responses.
* Cohere was fast and structured but limited in creativity.
* Meta AI delivered diverse and rich narrative responses.

### 13. Challenges Faced:
- Lack of consistent access to all platforms.
- Difficulty aligning API behaviors, as models may interpret prompts differently.
- Variations in output length and style across platforms, requiring further tuning for uniform results.
- 
### 14. Ethical Considerations:
- Prompting must avoid generating harmful, biased, or misleading content.
- It is essential to always review model output before deployment, especially in sensitive areas like healthcare or legal advice.

### 15. Result form various AI Chatbots:

#### Prompt : *Write a short sci-fi story set on Mars involving AI and human colonists*
##### ChatGPT:
![image](https://github.com/user-attachments/assets/0674ff68-ff9d-422d-8a8b-b2c0b170ae45)
![image](https://github.com/user-attachments/assets/66f02645-2265-44ca-a549-c78a5de5487d)

===========================================================================================

##### Gemini:
![image](https://github.com/user-attachments/assets/ab7fd7e3-2177-45b0-9a77-b78fd96339d3)
===========================================================================================

##### Claude AI:
![image](https://github.com/user-attachments/assets/aca36a88-8ca8-4347-9a9c-50fdfd032f91)

===========================================================================================

##### Grok AI:
![image](https://github.com/user-attachments/assets/3c0402d6-710b-4da5-ae51-12cb84235961)

===========================================================================================


### 16. Summary of Findings:
| Prompt Type         | Best Use Case       | Output Depth | Accuracy | Flexibility |
| ------------------- | ------------------- | ------------ | -------- | ----------- |
| Broad               | Creativity          | Low          | Medium   | High        |
| Refined             | Professional Use    | High         | High     | Medium      |
| Few-Shot            | Teaching/Examples   | Medium       | High     | Medium      |
| Chain-of-Thought    | Logical Reasoning    | Very High    | Very High| Low         |
| Zero-Shot           | Simple Tasks        | Low          | Medium   | High        |
---

### Conclusion:
This experiment confirms that different prompting techniques serve different use cases across AI platforms. Structured, detailed prompts enhance accuracy, depth, and relevance, while examples in few-shot and reasoning in chain-of-thought prompting significantly elevate the model’s performance. As Generative AI grows, mastering prompting strategies will be key for developers, educators, and enterprises alike.
